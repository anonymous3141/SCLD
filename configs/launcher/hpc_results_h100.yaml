# @package _global_
defaults:
- override /hydra/launcher: submitit_slurm

hydra:
# Hydra
  run:
    dir: logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.override_dirname}
  job:
    env_set: 
      WANDB_START_METHOD: thread
      NUMEXPR_MAX_THREADS: 8
    config:
      override_dirname:
        exclude_keys:
          - launcher
          - ckpt_file
          - wandb.project
          - wandb.id
          - algorithm.prior.learn_mean
          - algorithm.prior.learn_variance
          - unique_id
          - algorithm.model.num_hid
          - algorithm.model.inner_clip
          - algorithm.resampler.resampler_args
          - algorithm.n_sub_traj
          - algorithm.annealing_schedule.schedule_lr
          - algorithm.noise_schedule.sigma_min
          - algorithm.noise_schedule.s
          - algorithm.use_resampling
          - algorithm.use_markov
          - algorithm.buffer.max_length_in_batches
          - algorithm.buffer.sampling_scheme
          - target.cmcd.step_size
          - target.flow_transport.initial_scale
          - target.flow_transport.step_size
          - algorithm.num_temps
        kv_sep: .
        item_sep: "-"
  callbacks:
    log_job_return:
      _target_: hydra.experimental.callbacks.LogJobReturnCallback
  launcher:
    nodes: 1
    #gpus_per_node: 1
    cpus_per_task: 2
    timeout_min: 4320
    constraint:
    partition: gpu
    qos: 
    name: ${hydra.run.dir}/${hydra.job.override_dirname}
    max_num_timeout: 3
    stderr_to_stdout: true
    additional_parameters:
      gres: gpu:h100:1
      mem: 64GB  # Allocating 64GB of memory
